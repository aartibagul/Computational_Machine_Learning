{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import wave, struct, numpy as np, matplotlib.mlab as mlab, pylab as pl\n",
    "\n",
    "filename = \"CML_Recording_Both.wav\"\n",
    "w = wave.open(filename,\"rb\")\n",
    "\n",
    "#returns a named tuple (nchannels, sampwidth, framerate, \n",
    "# nframes, comptype, compname)\n",
    "waveParams = w.getparams()\n",
    "\n",
    "s = w.readframes(waveParams[3])\n",
    "w.close()\n",
    "waveArray = np.fromstring(s, np.int16)\n",
    "\n",
    "spectrum, freq, bins = mlab.specgram(waveArray, NFFT=256,Fs=waveParams[2],sides='onesided')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculates and returns spectrogram of \n",
    "def get_wav(filename)\n",
    "    filename = \"CML_Recording_Both.wav\"\n",
    "    w = wave.open(filename,\"rb\")\n",
    "    return w\n",
    "\n",
    "# spectrogram is the absolute value of the stft\n",
    "def get_sepctrogram(stft):\n",
    "    return abs(stft)\n",
    "\n",
    "# takes in wave file as input\n",
    "# win_size is the length of the window in samples\n",
    "# overlap is the amount of overlap between windows in samples\n",
    "def my_stft(w, win_size, overlap):\n",
    "    # number of samples in the wave file\n",
    "    n = len(w)\n",
    "    win_count= 0\n",
    "    \n",
    "\n",
    "def my_istft(stft):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 11263)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# divergence\n",
    "def div(v,W,h):\n",
    "    whv = np.dot(W,h) * 1/v\n",
    "    div = whv - np.log(whv) - 1\n",
    "    div = np.dot( div, np.ones(div.shape) )\n",
    "    return div\n",
    "\n",
    "# divergence gradient\n",
    "def div_grad(v,W,h):\n",
    "    grad = np.dot( 1/v - 1/(np.dot(W,h)) , W)\n",
    "    return grad\n",
    "\n",
    "# epsilon divergence\n",
    "def compute_obj(v,W,h,eps):\n",
    "    \n",
    "    whv = (np.dot(W,h) + eps)/(v + eps)\n",
    "    if any(whv < 0):\n",
    "        print(W)\n",
    "        print(h)\n",
    "        print(v)\n",
    "        print('whv: ' + str(whv))\n",
    "    #print('whv.shape: ' + str(whv.shape))\n",
    "    #print('np.dot(W,h): ' + str(np.dot(W,h)))\n",
    "    div = whv - np.log(whv) - 1\n",
    "    #print(div)\n",
    "    return np.sum( div )\n",
    "\n",
    "# epsilon divergence gradient\n",
    "def compute_grad(v,W,h,eps):\n",
    "    #print('compute_grad start')\n",
    "    #print('W.shape: ' + str(W.shape))\n",
    "    #print('h.shape: ' + str(h.shape))\n",
    "    #print('v.shape: ' + str(v.shape))\n",
    "    #print('np.dot(W,h).shape: ' + str( (np.dot(W,h) + eps).shape ) )\n",
    "    #print((1/(v + eps) - 1/(np.dot(W,h) + eps)).shape)\n",
    "    grad = np.dot(W.T, (1/(v + eps) - 1/(np.dot(W,h) + eps)))\n",
    "    #print('compute_grad end')\n",
    "    return grad\n",
    "\n",
    "def itakura_saito(y,x):\n",
    "    y = np.array(y)\n",
    "    x = np.array(x)\n",
    "    return np.sum(y/x - np.log(y/x) -1)\n",
    "    \n",
    "# important! input here has to be the matrix H, not the vector h_t\n",
    "# this is because we need h_t but also h_(t-1) and h_(t+1)\n",
    "# ind_t is the index of h_t in H\n",
    "# lambda is the smoothness constant\n",
    "def compute_smooth_obj(v,W,H,ind_t,lamb,eps):\n",
    "    \n",
    "    # get the column h_t\n",
    "    h = H[:,ind_t]\n",
    "    h = h.reshape(h.shape[0],1)\n",
    "    \n",
    "    # compute regular objective\n",
    "    # maybe doing this direct instead of the function call is faster:\n",
    "    # whv = (np.dot(W,h) + eps)/(v + eps)\n",
    "    # div = whv - np.log(whv) - 1 \n",
    "    div = compute_obj(v,W,h,eps)\n",
    "    \n",
    "    # compute smoothness terms\n",
    "    s1 = H[:,ind_t]/H[:,ind_t-1]\n",
    "    s2 = H[:,ind_t]/H[:,ind_t+1]\n",
    "    sm = s1 - np.log(s1) - 1\n",
    "    sm += s2 - np.log(s2) - 1\n",
    "    # returning properly scaled smooth objective\n",
    "    return div + lamb * np.sum( sm )\n",
    "    \n",
    "# input parameters as above\n",
    "def compute_smooth_grad(v,W,H,ind_t,lamb,eps):\n",
    "    \n",
    "    # get the column h_t\n",
    "    h = H[:,ind_t]\n",
    "    h = h.reshape(h.shape[0],1)\n",
    "    \n",
    "    # calculates gradient of regular divergence\n",
    "    div_grad = compute_grad(v,W,h,eps)\n",
    "\n",
    "    # calculating gradient of smoothness term\n",
    "    sm_grad = 1/H[:,ind_t-1] + 1/H[:,ind_t+1] - 2/H[:,ind_t]\n",
    "    sm_grad = sm_grad.reshape(sm_grad.shape[0],1)\n",
    "    \n",
    "    # returning properly scaled gradient of smooth objective \n",
    "    return div_grad + lamb * sm_grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.91200162]\n",
      " [ 0.42126031]]\n",
      "[[-0.91199808]\n",
      " [ 0.42126121]]\n"
     ]
    }
   ],
   "source": [
    "def grad_checker(v, W, h):\n",
    "    eps = 1e-3\n",
    "    (f,k) = W.shape\n",
    "    t_grad = np.zeros(h.shape)\n",
    "    for i in range(k):\n",
    "        ei = np.zeros(h.shape)\n",
    "        ei[i] = eps\n",
    "        t_grad[i] = (compute_obj(v,W,h+ei, 1e-12) - compute_obj(v,W,h-ei,1e-12)) / (2*eps)\n",
    "    print(t_grad)\n",
    "    print(compute_grad(v,W,h,1e-12))\n",
    "grad_checker(np.random.rand(2,1), np.random.rand(2,2),np.random.rand(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.53544566  9.5102638 ]\n",
      "[[ 7.53544693]\n",
      " [ 9.51026604]]\n"
     ]
    }
   ],
   "source": [
    "def grad_checker(v,W,H,ind_t,lamb,eps):\n",
    "    eps_dif = 1e-3\n",
    "    (f,k) = W.shape\n",
    "    t_grad = np.zeros(H[:,ind_t].shape)\n",
    "    for i in range(k):\n",
    "        ei = np.zeros(H.shape)\n",
    "        ei[i,ind_t] = eps_dif\n",
    "        t_grad[i] = (compute_smooth_obj(v,W,H+ei,ind_t,lamb,eps) - compute_smooth_obj(v,W,H-ei,ind_t,lamb,eps)) / (2*eps_dif)\n",
    "    print(t_grad)\n",
    "    print(compute_smooth_grad(v,W,H,ind_t,lamb,eps))\n",
    "grad_checker(np.random.rand(2,1), np.random.rand(2,2),np.random.rand(2, 3),1,1,1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_backtracking(v, W, h, max_iter, compute_grad, compute_obj, eps):\n",
    "    \n",
    "    v = v.reshape(v.shape[0],1)\n",
    "  \n",
    "    beta = 0.5 #backstep factor between 0.1 and 0.8\n",
    "    opt_prec = 1-1e-4 # optimization precision\n",
    "    n = 1e-1 #initial step size\n",
    "    \n",
    "    h = np.random.rand(2, 1)\n",
    "    \n",
    "    obj = [None]*max_iter\n",
    "    \n",
    "    max_backstep = 100 # maximum number of backsteps\n",
    "    t = 0 # backstepping counter\n",
    "    k = 0 # gradient step counter \n",
    "    \n",
    "    while( k < max_iter and t != max_backstep ):\n",
    "        \n",
    "        grad = compute_grad(v,W,h,eps)\n",
    "        obj[k] = compute_obj(v,W,h,eps)\n",
    "        \n",
    "        t = 0 # reset backstepping counter\n",
    "        n = 1/beta*n # try to increase stepsize slightly again\n",
    "        \n",
    "        # make sure h-n*grad is positive\n",
    "        while(any(h - n * grad < 0)  and t < max_backstep ):\n",
    "            t += 1\n",
    "            n = beta * n\n",
    "    \n",
    "        new_obj = compute_obj(v,W,(h - n*grad),eps)\n",
    "        while( new_obj > opt_prec * compute_obj(v,W,h,eps) and t < max_backstep):\n",
    "            t += 1\n",
    "            n = beta * n\n",
    "            new_obj = abs(compute_obj(v,W,(h - n*grad),eps))\n",
    "                      \n",
    "        h = h - n * grad # update h according to gradient step\n",
    "        k += 1 # update gradient step counter\n",
    "       \n",
    "   \n",
    "    return h, obj[0:k]\n",
    "\n",
    "h, obj = gradient_backtracking(np.random.rand(10,1), np.random.rand(10,2),  np.random.rand(2, 1), 100, compute_grad, compute_obj, 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25.294090530377957, 9.523756099388935, 7.9600867384393883, 7.6123309624643438, 7.5394195057403977, 7.5046593723963975, 7.4961605757929686]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEKCAYAAAAyx7/DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dcfacf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = [i for i in range(10)]\n",
    "plt.title(\"On training data\")\n",
    "plt.plot([i for i in range(len(obj))], obj)\n",
    "\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10397\n",
      "1159\n",
      "7673\n",
      "1195\n",
      "8925\n",
      "8117\n",
      "8352\n",
      "9888\n",
      "2726\n",
      "1144\n",
      "3475\n",
      "3826\n",
      "3032\n",
      "1861\n",
      "2194\n",
      "5991\n",
      "10329\n",
      "9339\n",
      "1516\n",
      "9884\n",
      "390\n",
      "7362\n",
      "9378\n",
      "7485\n",
      "6431\n",
      "9810\n",
      "8308\n",
      "8573\n",
      "5932\n",
      "9897\n",
      "3574\n",
      "3142\n",
      "7137\n",
      "10108\n",
      "6775\n",
      "3772\n",
      "6585\n",
      "5474\n",
      "6239\n",
      "5812\n",
      "1689\n",
      "5567\n",
      "2060\n",
      "7920\n",
      "5159\n",
      "9849\n",
      "6252\n",
      "3475\n",
      "8488\n",
      "3944\n",
      "10605\n",
      "7137\n",
      "5211\n",
      "3008\n",
      "9898\n",
      "3394\n",
      "6706\n",
      "1929\n",
      "1727\n",
      "5845\n",
      "1167\n",
      "5818\n",
      "5789\n",
      "5076\n",
      "8082\n",
      "7449\n",
      "9597\n",
      "7358\n",
      "9852\n",
      "8601\n",
      "6206\n",
      "15\n",
      "1331\n",
      "3413\n",
      "3760\n",
      "2098\n",
      "1738\n",
      "3532\n",
      "7715\n",
      "2317\n",
      "10439\n",
      "2162\n",
      "7252\n",
      "7377\n",
      "5980\n",
      "695\n",
      "9375\n",
      "11003\n",
      "6948\n",
      "7636\n",
      "4654\n",
      "3109\n",
      "1814\n",
      "10295\n",
      "3446\n",
      "(129, 2)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def online_nmf(spectrum, W, H,A, B, rho, beta, n, eps):\n",
    "   \n",
    "    a = np.zeros(W.shape)\n",
    "    b = np.zeros(W.shape)\n",
    "\n",
    "    t = 1\n",
    "    W_old = W\n",
    "    k = W.shape[1]\n",
    "    h = np.random.rand(W.shape[1], 1)\n",
    "    h_old = h\n",
    "    \n",
    "    while np.linalg.norm(W - W_old, ord = \"fro\") < n:\n",
    "        \n",
    "        t = t+1 \n",
    "        \n",
    "        ind = random.randint(0, len(spectrum.T)-1)\n",
    "        print(ind)\n",
    "        v = spectrum.T[ind]\n",
    "        h_old = h\n",
    "        h, obj = gradient_backtracking(v, W, h_old, 100, compute_grad, compute_obj, 1e-12)\n",
    "   \n",
    "        h = h.reshape(h.shape[0],1)\n",
    "        v = v.reshape(v.shape[0],1)\n",
    "        den = eps + np.dot(W, h)\n",
    "        \n",
    "        a += np.dot(np.dot(((eps+v)/(den**2)), h.T), np.dot(W.T,W))\n",
    "        b += np.dot(1/den, h.T)\n",
    "        \n",
    "        if t % beta == 0:\n",
    "            A = A + rho*a\n",
    "            a = 0\n",
    "            B = B + rho*b\n",
    "            b = 0\n",
    "            W_old = W\n",
    "            W = np.sqrt(A/B)\n",
    "            \n",
    "            W = np.array([x/sum(x) for x in zip(*W)]).T\n",
    "            A = np.array([x/sum(x) for x in zip(*A)]).T\n",
    "            B = np.array([x*sum(x) for x in zip(*B)]).T\n",
    "            \n",
    "        if t > 95:\n",
    "            print(W.shape)\n",
    "            break\n",
    "\n",
    "eps = 1e-12\n",
    "v = spectrum.T[0]\n",
    "K = 2\n",
    "W = np.random.rand(spectrum.shape[0],K)\n",
    "H = np.zeros((K, spectrum.shape[1]))\n",
    "\n",
    "A = np.zeros(W.shape)\n",
    "B = np.zeros(W.shape)\n",
    "\n",
    "online_nmf(spectrum, W, H, A, B, 0.5, 100, 1e-3, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps = 1e-12\n",
    "random.seed(12222015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = [sum(x) for x in zip(*W)]\n",
    "W = [sum(x) for x in zip(*W)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 129)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = np.array([[1,1], [1,2], [2,2]])\n",
    "X = [1,2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 2],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25  0.25  0.5 ]\n",
      "[0 0 0]\n",
      "[ 0.2  0.4  0.4]\n",
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X = W.T\n",
    "for i in range(2):\n",
    "    \n",
    "    col_sum = X[i].sum()\n",
    "    print(X[i]/col_sum)\n",
    "    X[i] = (X[i]/col_sum)\n",
    "    print(X[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = np.array([x/sum(x) for x in zip(*W)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25,  0.2 ],\n",
       "       [ 0.25,  0.4 ],\n",
       "       [ 0.5 ,  0.4 ]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# divergence\n",
    "def div(v,W,h):\n",
    "    whv = np.dot(W,h) * 1/v\n",
    "    div = whv - np.log10(whv) - 1\n",
    "    div = np.dot( div, np.ones )\n",
    "    return div\n",
    "\n",
    "# divergence gradient\n",
    "def div_grad(v,W,h):\n",
    "    grad = np.dot( 1/v - 1/(np.dot(W,h)) , W)\n",
    "    return grad\n",
    "\n",
    "# epsilon divergence\n",
    "def eps_div(v,W,h,eps):\n",
    "    whv = (np.dot(W,h) + eps) * 1/(v + eps)\n",
    "    div = whv - np.log10(whv) - 1\n",
    "    div = np.dot( div, np.ones )\n",
    "    return div\n",
    "\n",
    "# epsilon divergence gradient\n",
    "def eps_div_grad(v,W,h,eps):\n",
    "    grad = np.dot( 1/(v + eps) - 1/(np.dot(W,h) + eps), W)\n",
    "    return grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
